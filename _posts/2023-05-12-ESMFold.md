---
title:  "Evolutionary-scale prediction of atomic-level protein structure with a language model(2023)"
classes: wide
date:   2023-05-12 17:29:03 +0900
categories: 
  - proteinstructure
tags:
  - language-model

---

## Intrdocution

The biological properties of a protein constrain the mutations to its sequence that are selected through evolution, recording biology into evolutionary patterns. Beginning with Shannon's model for the entorpy of text, language models of increading complexiry have been developed, which has culminated in modern large-scale attention based architectures.We posit that the task of filling in missing amino acids in protein sequences across evolution will require a language model to understand the underlying structure that creates the patterns in the sequences. Strong correlations betwen the language model's understanding of the protein sequence(perplexity) and the accuracy of the structure prediction revela a close link between language modeling and the learning of structure. 

We show that language models enable fast end-to-end atomic-resolution structure prediction directly from sequence. Our approach leverages the evolutionary patterns that are captured by the language model to produce accurate atomic-level predictions. This removes costly aspects of the current state-of-the-art structure prediction pipeline, which eliminates the need for a multiple sequence alignment(MSA) while greatly simplifying the neural architecture used for inference. 

## Materials and Methods

### Data

UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% sequences to form the validation set. All train sequences which match a validation sequence with 50% sequence identitu under this search are removed from the train set. The 50% identity threshold is chosen because the purpose of the validation set is primarily to detect overfitting, rather than to test generalization. 

We find all PDB chains until 2020-05-01 with resolution less than or equal to 9$$\mathring{A}$$and length greater than 20. All proteins where over 20% of the sequence is the same residue is not considered. MMseqs easy-cluster with default parameters is used to cluster resulting sequences at 40% sequence identity. Only individual chains are used during training, even when the chain is part of a protein complex. This results in 25,450 clusters covering a total fo 325,498 chains. At training time, each cluster is sampled evenly, and then a random protein is sampled from each cluster. Rejection sampling is applied to train on longer proteins more frequently, where protein chains are accepted with probability $$ \dfrac{1}{512} \text{max}(min(N_{\text{res}},512),256).

### Language Models

#### Computing unsupervised contact prediction from language models

We used the methodology of Rao et al. [[1]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.abstract) to measure unsupervised learning of tertiary structure in the form of contact maps. A logistic regression is used to identify contacts. The probability of a contact is defined as $$p(c_{ij}) = (1+\text{exp}(-\beta_{0}-\Sigma_{l=1}^{N}\Sigma_{k=1}^{K} \beta_{kl} \alpha_{ij}^{kl}))^{-1}$$



## Atomic-resolution structure emerges in language models trained on protein sequences



## References

1. Rao, Roshan, et al. "Transformer protein language models are unsupervised structure learners." *Biorxiv* (2020): 2020-12.

