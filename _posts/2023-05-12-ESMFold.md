---
title:  "Evolutionary-scale prediction of atomic-level protein structure with a language model(2023)"
classes: wide
date:   2023-05-12 17:29:03 +0900
categories: 
  - proteinstructure
tags:
  - language-model

---

## Intrdocution

The biological properties of a protein constrain the mutations to its sequence that are selected through evolution, recording biology into evolutionary patterns. Beginning with Shannon's model for the entorpy of text, language models of increasing complexity have been developed, which has culminated in modern large-scale attention based architectures.We posit that the task of filling in missing amino acids in protein sequences across evolution will require a language model to understand the underlying structure that creates the patterns in the sequences. Strong correlations betwen the language model's understanding of the protein sequence(perplexity) and the accuracy of the structure prediction revela a close link between language modeling and the learning of structure. 

We show that language models enable fast end-to-end atomic-resolution structure prediction directly from sequence. Our approach leverages the evolutionary patterns that are captured by the language model to produce accurate atomic-level predictions. This removes costly aspects of the current state-of-the-art structure prediction pipeline, which eliminates the need for a multiple sequence alignment(MSA) while greatly simplifying the neural architecture used for inference. 

## Materials and Methods

### Data

UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% sequences to form the validation set. All train sequences which match a validation sequence with 50% sequence identitu under this search are removed from the train set. The 50% identity threshold is chosen because the purpose of the validation set is primarily to detect overfitting, rather than to test generalization. 

We find all PDB chains until 2020-05-01 with resolution less than or equal to 9$$\mathring{A}$$and length greater than 20. All proteins where over 20 $$%$$ of the sequence is the same residue is not considered. MMseqs easy-cluster with default parameters is used to cluster resulting sequences at 40 $$% $$ sequence identity. Only individual chains are used during training, even when the chain is part of a protein complex. This results in 25,450 clusters covering a total fo 325,498 chains. At training time, each cluster is sampled evenly, and then a random protein is sampled from each cluster. Rejection sampling is applied to train on longer proteins more frequently, where protein chains are accepted with probability $$ \dfrac{1}{512} \text{max}(min(N_{\text{res}},512),256)$$.

### Language Models

#### Computing unsupervised contact prediction from language models

We used the methodology of Rao et al. [[1]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.abstract) to measure unsupervised learning of tertiary structure in the form of contact maps. A logistic regression is used to identify contacts. The probability of a contact is defined as $$p(c_{ij}) = (1+\text{exp}(-\beta_{0}-\Sigma_{l=1}^{N}\Sigma_{k=1}^{K} \beta_{kl} \alpha_{ij}^{kl}))^{-1}$$ where $$c_{ij}$$ is a boolean random variable which is true if amino acids i,j are in contact. Suppose our transformer has N layers and K attention heads per layer. Then $$\beta_{kl}$$ is symmetrized and APC-corrected [[2]](https://academic.oup.com/bioinformatics/article/24/3/333/253952) attention map for the k-th attention head in the l-th layer of the transformer, and $$\alpha_{ij}^{kl}$$ is the value of that attention map at position i,j. 

The metric we use, long range P@L, for each protein, is defined as the precision of the top L predicted long range contacts by confidence for a protein of length L.  long range is defined as contacts that are more than 24 residues apart in the protein sequence. This is averaged over each protein that we test over. We also use P@L/5 in some sections of this work, which computes precision over the top L/5 predictions instead. 

#### ESM-2 model architecture

We use a BERT style encoder only transformer architecture with modifications. We change the number of layers, number of attention heads, hidden size and feed forward hidden size as we scael the ESM model. 

![fig1](https://jasonkim8652.github.io/assets/images/ESM_1.PNG)

The original transformer paper uses absolute sinusoidal positional encoding to inform the model about token positions. These positional encodings are added to the input embeddings at the bottom of the encoder stack. In ESM-1b, we replaced this static sinusoidal encoding with a learned one. Both static and learned absolute encodings provide the model a very cheap way of adding positional information. However, absolute positional encoding methods don't extrapolate well beyond the context window they are trained on. 

> Learned positional embedding makes it extrapolate better. 

In ESM-2, we used Rotary Position Embedding(RoPE) [[3]](https://arxiv.org/abs/2104.09864) to allow the model to extrpolate beyond the context window it is trained on. RoPE slightly increases the computational cost of the model since it multiplies every query and key vector inside the self-attention with a sinusoidal embedding. In our experiments, we observed that this improves model quality for small models. However, we observed that the performance improvements start to diappears as the model size and training duration get bigger. 

![fig2](https://jasonkim8652.github.io/assets/images/ESM_2.PNG)

#### Relationship between change in perplexity and contact accuracy

The relationship between improvements in perplexity and improvements in contact accuracy can be measured via normalized discounted cumulative gain(NDCG). In particular, we hypothesize that large improvements in perplexity correspond with large improvements in contact accuracy. 



## Atomic-resolution structure emerges in language models trained on protein sequences



## References

1. Rao, Roshan, et al. "Transformer protein language models are unsupervised structure learners." *Biorxiv* (2020): 2020-12.
2. Dunn, Stanley D., Lindi M. Wahl, and Gregory B. Gloor. "Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction." *Bioinformatics*24.3 (2008): 333-340.
3. Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." arXiv preprint arXiv:2104.09864 (2021).

