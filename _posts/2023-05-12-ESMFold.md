---
title:  "Evolutionary-scale prediction of atomic-level protein structure with a language model(2023)"
classes: wide
date:   2023-05-12 17:29:03 +0900
categories: 
  - proteinstructure
tags:
  - language-model

---

## Intrdocution

The biological properties of a protein constrain the mutations to its sequence that are selected through evolution, recording biology into evolutionary patterns. Beginning with Shannon's model for the entorpy of text, language models of increasing complexity have been developed, which has culminated in modern large-scale attention based architectures.We posit that the task of filling in missing amino acids in protein sequences across evolution will require a language model to understand the underlying structure that creates the patterns in the sequences. Strong correlations betwen the language model's understanding of the protein sequence(perplexity) and the accuracy of the structure prediction revela a close link between language modeling and the learning of structure. 

We show that language models enable fast end-to-end atomic-resolution structure prediction directly from sequence. Our approach leverages the evolutionary patterns that are captured by the language model to produce accurate atomic-level predictions. This removes costly aspects of the current state-of-the-art structure prediction pipeline, which eliminates the need for a multiple sequence alignment(MSA) while greatly simplifying the neural architecture used for inference. 

## Materials and Methods

### Data

UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% sequences to form the validation set. All train sequences which match a validation sequence with 50% sequence identitu under this search are removed from the train set. The 50% identity threshold is chosen because the purpose of the validation set is primarily to detect overfitting, rather than to test generalization. 

We find all PDB chains until 2020-05-01 with resolution less than or equal to 9$$\mathring{A}$$and length greater than 20. All proteins where over 20 $$%$$ of the sequence is the same residue is not considered. MMseqs easy-cluster with default parameters is used to cluster resulting sequences at 40 $$% $$ sequence identity. Only individual chains are used during training, even when the chain is part of a protein complex. This results in 25,450 clusters covering a total fo 325,498 chains. At training time, each cluster is sampled evenly, and then a random protein is sampled from each cluster. Rejection sampling is applied to train on longer proteins more frequently, where protein chains are accepted with probability $$ \dfrac{1}{512} \text{max}(min(N_{\text{res}},512),256)$$.

### Language Models

#### Computing unsupervised contact prediction from language models

We used the methodology of Rao et al. [[1]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.abstract) to measure unsupervised learning of tertiary structure in the form of contact maps. A logistic regression is used to identify contacts. The probability of a contact is defined as $$p(c_{ij}) = (1+\text{exp}(-\beta_{0}-\Sigma_{l=1}^{N}\Sigma_{k=1}^{K} \beta_{kl} \alpha_{ij}^{kl}))^{-1}$$ where $$c_{ij}$$ is a boolean random variable which is true if amino acids i,j are in contact. Suppose our transformer has N layers and K attention heads per layer. Then $$\beta_{kl}$$ is symmetrized and APC-corrected [[2]](https://academic.oup.com/bioinformatics/article/24/3/333/253952) attention map for the k-th attention head in the l-th layer of the transformer, and $$\alpha_{ij}^{kl}$$ is the value of that attention map at position i,j. 

The metric we use, long range P@L, for each protein, is defined as the precision of the top L predicted long range contacts by confidence for a protein of length L.  long range is defined as contacts that are more than 24 residues apart in the protein sequence. This is averaged over each protein that we test over. We also use P@L/5 in some sections of this work, which computes precision over the top L/5 predictions instead. 

#### Language model perplexity calculations

Perplexity is a measure of a language model's fidelity and is defined as the exponential of the negative log-likelihood of the sequence. Unfortunately, there is no efficient method of computing the log-likelihood of a sequence under a masked language model. Instead, there are two methods we can use for estimating perplexity. 

First, let the mask M be a random variable denoting a set of tokens from input sequence x. Each token has a probability of inclusion. 

## Atomic-resolution structure emerges in language models trained on protein sequences



## References

1. Rao, Roshan, et al. "Transformer protein language models are unsupervised structure learners." *Biorxiv* (2020): 2020-12.
2. Dunn, Stanley D., Lindi M. Wahl, and Gregory B. Gloor. "Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction." *Bioinformatics*24.3 (2008): 333-340.

