---
title:  "Evolutionary-scale prediction of atomic-level protein structure with a language model(2023)"
classes: wide
date:   2023-05-12 17:29:03 +0900
categories: 
  - proteinstructure
tags:
  - language-model

---

## Intrdocution

The biological properties of a protein constrain the mutations to its sequence that are selected through evolution, recording biology into evolutionary patterns. Beginning with Shannon's model for the entorpy of text, language models of increasing complexity have been developed, which has culminated in modern large-scale attention based architectures.We posit that the task of filling in missing amino acids in protein sequences across evolution will require a language model to understand the underlying structure that creates the patterns in the sequences. Strong correlations betwen the language model's understanding of the protein sequence(perplexity) and the accuracy of the structure prediction revela a close link between language modeling and the learning of structure. 

We show that language models enable fast end-to-end atomic-resolution structure prediction directly from sequence. Our approach leverages the evolutionary patterns that are captured by the language model to produce accurate atomic-level predictions. This removes costly aspects of the current state-of-the-art structure prediction pipeline, which eliminates the need for a multiple sequence alignment(MSA) while greatly simplifying the neural architecture used for inference. 

## Materials and Methods

### Data

UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% sequences to form the validation set. All train sequences which match a validation sequence with 50% sequence identitu under this search are removed from the train set. The 50% identity threshold is chosen because the purpose of the validation set is primarily to detect overfitting, rather than to test generalization. 

We find all PDB chains until 2020-05-01 with resolution less than or equal to 9$$\mathring{A}$$and length greater than 20. All proteins where over 20 $$%$$ of the sequence is the same residue is not considered. MMseqs easy-cluster with default parameters is used to cluster resulting sequences at 40 $$% $$ sequence identity. Only individual chains are used during training, even when the chain is part of a protein complex. This results in 25,450 clusters covering a total fo 325,498 chains. At training time, each cluster is sampled evenly, and then a random protein is sampled from each cluster. Rejection sampling is applied to train on longer proteins more frequently, where protein chains are accepted with probability $$ \dfrac{1}{512} \text{max}(min(N_{\text{res}},512),256)$$.

### Language Models

#### Computing unsupervised contact prediction from language models

We used the methodology of Rao et al. [[1]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.abstract) to measure unsupervised learning of tertiary structure in the form of contact maps. A logistic regression is used to identify contacts. The probability of a contact is defined as $$p(c_{ij}) = (1+\text{exp}(-\beta_{0}-\Sigma_{l=1}^{N}\Sigma_{k=1}^{K} \beta_{kl} \alpha_{ij}^{kl}))^{-1}$$ where $$c_{ij}$$ is a boolean random variable which is true if amino acids i,j are in contact. Suppose our transformer has N layers and K attention heads per layer. Then $$\beta_{kl}$$ is symmetrized and APC-corrected [[2]](https://academic.oup.com/bioinformatics/article/24/3/333/253952) attention map for the k-th attention head in the l-th layer of the transformer, and $$\alpha_{ij}^{kl}$$ is the value of that attention map at position i,j. 

The metric we use, long range P@L, for each protein, is defined as the precision of the top L predicted long range contacts by confidence for a protein of length L.  long range is defined as contacts that are more than 24 residues apart in the protein sequence. This is averaged over each protein that we test over. We also use P@L/5 in some sections of this work, which computes precision over the top L/5 predictions instead. 

#### ESM-2 model architecture

We use a BERT style encoder only transformer architecture with modifications. We change the number of layers, number of attention heads, hidden size and feed forward hidden size as we scael the ESM model. 

![fig1](https://jasonkim8652.github.io/assets/images/ESM_1.PNG)

The original transformer paper uses absolute sinusoidal positional encoding to inform the model about token positions. These positional encodings are added to the input embeddings at the bottom of the encoder stack. In ESM-1b, we replaced this static sinusoidal encoding with a learned one. Both static and learned absolute encodings provide the model a very cheap way of adding positional information. However, absolute positional encoding methods don't extrapolate well beyond the context window they are trained on. 

> Learned positional embedding makes it extrapolate better. 

In ESM-2, we used Rotary Position Embedding(RoPE) [[3]](https://arxiv.org/abs/2104.09864) to allow the model to extrpolate beyond the context window it is trained on. RoPE slightly increases the computational cost of the model since it multiplies every query and key vector inside the self-attention with a sinusoidal embedding. In our experiments, we observed that this improves model quality for small models. However, we observed that the performance improvements start to diappears as the model size and training duration get bigger. 

![fig2](https://jasonkim8652.github.io/assets/images/ESM_2.PNG)

#### Relationship between change in perplexity and contact accuracy

The relationship between improvements in perplexity and improvements in contact accuracy can be measured via normalized discounted cumulative gain(NDCG). In particular, we hypothesize that large improvements in perplexity correspond with large improvements in contact accuracy. We define the change in perplexity as the difference in language model perplexity for a particular protein sequence between adjacent model sizes. Similarly, we define the change in contact accuracy as the difference in unsupervised contact precision for a particular protein sequence between adjacent model sizes. We then compute the NDCG with respect to the change in contact accuracy. The average NDCG across the five model classes is 0.87.

### ESM Fold

#### ESMFold model architecture

![fig3](https://jasonkim8652.github.io/assets/images/ESM_3.PNG)

The ESMFold model uses a simple architecture that leverages the evolutionary information captured by the language model. The architecture is split into two parts, similarly to AF2: a folding module which takes the language model features as input and produces representations, and a structure module which takes the language model features as input and produces representations, and a structure module which takes the input from the folding module and outputs 3d atomic coordinates. For the structure module, we use the equivariant transformer architecture with invariant point attentiona proposed in AF2. For the folding block we simplify the Evoformer block used in AF2. No templates are used in ESMFold. 

The major change that needs to be made to adapt the Evoformer block to language model features is to remove its dependence on MSAs. Since MSAs are two dimensional, the Evoformer employs axial attention over the columns and rows of the MSA. **The language model features are one dimensional, so we can replace the axial attention with a standard attention over this feature space.** The self-attention uses a bias derived from the pairwise representations. The sequences representation communicates with pairwise representation via both an outer product and outer difference. Other operations in the Evoformer block are kept the same. 

We use a learned weighted sum of ESM embeddings to produce the initial hidden state into the model. This is then fed through a MLP. The initial pairwise state is simply the pairwise relative positional encoding described in AF. We found that using the attention maps initially gives a boost in performance, but this disappears during training. For experiments that do not use any folding blocks, we use an MLP applied to the ESM attention maps as input and add the pairwise relative positional encoding to the attention map scores. Finally, the structure module projects these representations into coordinates. 

The predicted LDDT head is output from the hidden representation of the structuremodule. The predicted TM head uses the pairwise representation z. Finally we also preict the distogram, from the same representation.

#### Masked prediction

It is possible to sample alternate predictions from ESMFold by masking inputs to the language model. We test this procedure with the following protocol: Input 1000 different sequences into ESMFold with different masking patterns inthe language model. The masking patterns are uniformly sampled, where 0 to 15% of the sequence is masked out. A prediction is made for each masked sequence, and the sequence with highest pLDDT is chosen as the final model prediction. On average, applyting this procedure only results in a 0.021 LDDT increases on CAMEO, but on some PDBs can substantially improve the accuracy.

## Atomic-resolution structure emerges in language models trained on protein sequences

Relative to our previous generation model ESM-1b, ESM-2 introduces improvements in architecture, training parameters, and increases computational resources and data. ESM-2 is trained to predict the identity of amino acids that have been randomly masked out of protein sequences:

![fig4](https://jasonkim8652.github.io/assets/images/ESM_4.PNG)

Although the training objective itself is simple and unsupervised, solving it over millions of evolutionarily diverse protein sequences requires the model to internalize sequence patterns across evolution. This training also results in the emergence of structure in the models. Because ESM-2's training is only on sequences, any information about structure that develops must be the result of representing the patterns in sequences. Transformer models that are trained with masked language modeling are known to develop attention patterns that correspond to the residue-residue contact map of the protein. We examine how this low-resolution picture of protien structure emerges as a function of scale. 

We discover that an atomic-resolution structure prediction can be projected from the representations of the ESM-2 language models. The accuracy of this projection improves with the scale of the language model. The 15 billion parameter model reaches a TM-score of 0.72 on the CAMEO test set and 0.55 on the CASP14 test set, a gain of 14 and 17% respectively relative to the 150 million parameter ESM-2 model. 

## References

1. Rao, Roshan, et al. "Transformer protein language models are unsupervised structure learners." *Biorxiv* (2020): 2020-12.
2. Dunn, Stanley D., Lindi M. Wahl, and Gregory B. Gloor. "Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction." *Bioinformatics*24.3 (2008): 333-340.
3. Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." arXiv preprint arXiv:2104.09864 (2021).

