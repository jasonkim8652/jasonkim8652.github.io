---
title:  "AbLang: an antibodyh language model for completing antibody sequences"
classes: wide
date:   2023-04-05 13:31:24 +0900
categories: 
  - immunogenicity
tags:
  - Humanization
---

## Introduction

Over the last decase, billions of antibodies have been sequenced. However, in some cases, the sequenced antibodies are missing residues due either to sequencing errors, such as ambiguous bases, or the limitations of the seuqnecing techniques used. **We find in OAS that ~80% of the seqeunces are missing more than one residue at the N-terminus and ~43% are missing the first 15 positions, and ~1% contain at least one ambiguous residue somewhere in the sequence. 

Here, we present AbLang, an antibody specific languge model trained on either the heavy or light chain antibody sequences from OAS. While AbLang can be used to create representations for residue or sequence sepcific predictions and residue engineering, in this paper, we focus on showing how AbLang can be used to restore missing resdiues in antibody sequences, more accurately than using IMGT germlines or a general protein model like ESM-1b

## Materials and methods

Two AbLang models were trained, one for heavy and one for light chains. Each AbLang model consists of two parts: AbRep, which creates representations from antibody sequences, and AbHead, which uses the representations to predict the likelihood of each amino acid at each position. 

![fig1](https://jasonkim8652.github.io/assets/images/Ablang_1.PNG)

AbRep follows the architecture of RoBERTa [[1]](https://arxiv.org/abs/1907.11692), except it uses a learned positional embedding layer with a max length of 160. Each of its 12 transformer blocks has 12 attenuated heads, an inner hidden size of 3072 and a hidden size of 768. From AbRep, the rescodings(768 values for each residue) are obtained. AbHead follows the design of RoBERTa's head model, with a hidden size of 768. 

During training, between 1% and 25% residues from each sequence were selected, and of these, 80% were masked, 10% randomly changed to another residue and 10% left unchanged.

## Results
### Data preparation
All antibody sequences seen three or more times in the OAS database as of October 2021 were downloaded. The heavy and light chain sequences were then clustered separately based on identical CDR3s and thereafter clustered further by 70% identity over the whole sequence using Linclust[[2]](https://www.nature.com/articles/s41467-018-04964-5), with the longest sequence selected from each cluster. The selected sequences were then randomly divided into training sets of 14,126,724 heavy and 187,068 light sequences, and two evaluation sets of 100,000 heavy and 50,000 light sequences. 

### Ablang's antibody seqeunce representations
Ablang can be used to generate three different sets of antibody seqeunce representations. The first representation, the res-codings, consists of 768 values for each residue, useful for residue speicific predictions. The second representatin, the seq-codings, represents the whole sequence and is derived from the mean of all res-codings in a sequence. The seq-coding have the benefit of having the same length for each sequence, removing the need to align antibody sequences. Lastly, AbLang can be used to generate the likelihoods of each amino acid at each position in a given antibody sequence, useful for antibody engineering. 
![fig2](https://jasonkim8652.github.io/assets/images/Ablang_2.PNG)

As above figure shows, AbLang and ESM-1b both seperate the sequences based on their V-gene familites; however, AbLang separates the V-genes into smaller clusters. These smaller clusters can partly be attributed to AbLang's finer seperation of V-genes. Within the AbLang clusters, a clearer separation can be seen between naive B-cells and memory B-cells than with ESM-1b's clusters. Further, the memory B-cells, in AbLang's clusters, appear to be ordered based on a gradual increase in mutations. This potentially indicates that AbLang representations contain information about the order of antibody mutations. 




## References

1. Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." arXiv preprint arXiv:1907.11692 (2019).
2. Steinegger, Martin, and Johannes SÃ¶ding. "Clustering huge protein sequence sets in linear time." Nature communications 9.1 (2018): 2542.