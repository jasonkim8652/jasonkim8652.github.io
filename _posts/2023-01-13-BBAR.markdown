---
title:  "Molecular Generative Model via Retrosyntheyically prepared Chemical Building Block Assembly"
classes: wide
excerpt: "Paper review about making synthesizable molecule generative model"
date:   2023-01-13 18:01:24 +0900
categories: 
  - SmallMolecule
tags:
  - Generative model
  - Graph based model
  - Synthesizability
mathjax: true
---
## Motivation

Despite Molecular generative model has promising results, their common design strategy does not explain synthesizability, which is important in developing chemotherpies. Human experts mostly perceive a molecule as a conneceted set of chemical building blocks rather than a simple assembly of atoms. Therefore, the direct consideration of synthesizability can be done by preparing synthetically accessible building blocks using known reaction templates and letting the model learn the implied synthetic validity from the resulting data. 
> Important point is 1. How to prepare Building blocks 2. How to teach model chemical Reaction. 

In this paper, they propose a novel deep generative model for molecular generation, namely building block-based autoregressive generative model (BBAR) which aims to design new molecules with target properties by sequentially adding building blocks to any given starting molecule. 

They used **Synthon-based molecule generation** based on **fragment-based molecule design**.

There are a few fragmeny-based molecular generative models, thought the definition of fragments in those works is different from the building block in this work. Podda et al.[[1]](https://proceedings.mlr.press/v108/podda20a.html) proposed a language model which sequentially generates fragements and combining them into a single molecule. They could achieve the high validity and uniqueness of generted molecules. Yang et al. [[2]](https://proceedings.neurips.cc/paper/2021/hash/41da609c519d77b29be442f8c1105647-Abstract.html) developed a reinfrocement learning model that sequentially adds fragments to a given core molecule to improve the binding affinity of the resulting molecule to a target protein. Desptie the conceptual advance of these models, they have fundamental limitations in dealing with diverse fragments. Yang et al. sampled fragments from a predefinced library which contains only 66 fragments. Podda et al. explicitly considered only a small number of frequent molecular fragments in a dataset. Furthermore, these modesl cannot accept novel fragments that are not in the training set, because they used fixed libraries.
This paper resolve these problems by splitting the building block selection process into two steps. We first sample a building block randomly from a predefined library. Then, we determine whether the sampled building block will be added to a given core molecule,which can be done using a deep neural network that predicts the probability of connecting between the given molecule and the building block. The neural network can take any building blocks as an embedding vector obtained by encoding the molecules using another deep neural network, so one can add new building blocks in the library without retraining the model. This strategy allows us to handle an unlimited number of building blocks in theory while maintaining high computational efficiency.  

## Method

![Tux, the figure 1](https://user-images.githubusercontent.com/59328209/212456986-9b37a2dd-1b89-4dc6-ab61-6f6ecfb53a81.jpg){: .center}

### Dataset
The library should contain all building blocks of the molecuels in the training set, preferably many molecules, so that the model could learn chemical diversity. There would be various definitions of building blocks such as BRICS, RECAP, Bemis-Murcko, and so on. In this work, the BRICS decomposition was adopted. The algorithm of the BRICS decomposition was breaking the covalent bonds which corresponded to predefined SMILES arbitrary target specification (SMARTS) strings. These SMARTS strings corresponded to synthetically accessible bonds, so BRICS decomposition was similar to retrosynthesis. 

### Building Block Selection Model
The Selection module took two molecular graphs from core molecule, building blocks and condition. The target properties were the properties of the original molecule in the training process. Then module predicted the probability of binding core graph and block graph. In this work, both of them were the molecular graph of a core molecule given as input of from the previous step and a building block sampled randomly from the building block library, respectively. We used graph convolution network. 

By applying the graph convolution layers, the two respecive graph vectors were obtained. To train the conditional model, molecular properties were incorporated into the condition vectors of Core graph. The probability value for building block selection pblock was evalueated as a function of core embedded vector and block embedded vector concatenation. 

The training dataset made by the BRICS decomposition intrinsically included only positive samples, wher "positive" means that building blocks of the same molecule prepared by the decomposition method. To make the model select such building blocks over others, it was also needed to train the model with namely the negative samples which were unlikely to be added. To prepare the negative samples for each positive sample, ten buildingblocks proportional to their occurence to the power of 3/4 as proposed in ref.[[3]](https://arxiv.org/abs/1301.3781) was randomly choosen. The model was trained to predict the building block probability as 1.0 for the positive samples and 0.0 for the negative samples. The binary cross-entropy loss was used for this task.

> Specific point of this paper is that they prepare negative fragment sample.

### Atom Selection Model
The atom selection module predicted the bonding probability between core graph node and block graph node. We only need to choose the block graph node. The atom selection module accepted output node embedding vectors from the building block selection module and applied the graph convolution layers to them. After applying the graph convolutional layers, the connection probability of core graph node was calculated using fully connected layers with a softmax layer at the end. The model was trained to predict connection probability as 1 for a positive atom and 0 for negative atoms. The cross-entropy loss for this task was used. 

### Termination Prediction Model
The termination prediction module predicted the probability of terminating the generation process. The module produced core graph vector by applying the graph convolution layers. Then, the termination prbability can be evaluated by applyting fully connected layers with the sigmoid activation function to the core graph vector. In the training process, the module was trained to predict the termination probability as 1.0 when the original molecules were recovered and other wise as 0.0.

### Moleculare Generation after Training the Model
The model accepted a starting molecule as input to generate a larger molecule by adding building blocks to it. If the starting molecule was not given, a building block was randomly selected from the building block library as a starting molecule. The conditional molecule generation additionally needed target properties as input. The generation process began to predict the termination probability of the starting molecule. Then, the termination sign was sampled in proportion to the termination probability. 

If not terminating, the model executed the building block selection and the atom selection modules subsequently. ** In the building block selection step, the model randomly samples a set of building blocks proportional to their populations in the training. This stochastic sampling enhanced the efficiency of the generation process. ** 

> How about clustering when selecting building blocks?

The number of building blocks in each sampling was a hyper-parameter. 

### What I've learned
* We could use synthon based method, not using reaction template. This method could solve the harsh subgraph isomorphism problem. 
* There are various building block generation method. Let's study about BRICS.
* Building block selection model, Atom selection model, termination prediction model. These are elaborate architecture as I think. We can use this architecture later. 
* Preparing negative building block was really impressive. This kind of detail is what we should learn.  

## Reference
1. Podda, Marco, Davide Bacciu, and Alessio Micheli. "A deep generative model for fragment-based molecule generation." International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
2. Yang, Soojung, et al. "Hit and lead discovery with explorative rl and fragment-based molecule generation." Advances in Neural Information Processing Systems 34 (2021): 7924-7936.
3. Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).