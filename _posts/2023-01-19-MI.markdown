---
title:  "Multiplicative Interactions and Where to Find Them"
classes: wide
excerpt: "Paper review about Multiplicative interaction, which is new concept for fusing representation"
date:   2023-01-18 10:31:24 +0900
categories: 
  - AI
tags:
  - AI background
mathjax: true
---

## Introduction

> Concatenation has some problem in their expressive power when we handle two different vector. Let's study about Multiplicaitve implication that could increase expressibe power of model. 

Multiplicative interactions have reappeared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We hypothesize that multiplicative interactions are suitable for representing certain meaningufl classes of functions needed to build algorithmic operations such as conditional statements or simlarity metrics, and more generally as an effective way of intergrating contextual infromation in a network in a way that generalized effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challengning, large-scale reinforcement learning(RL) and sequence modelling tasks when a conceptually simple muiltiplicative interaction module is incorporated. 

Such improvements are consistent with our hypothesis that  the use of appropriately applied multiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. 

Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models(hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficiacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems. 

## Multiplicative Interactions
The underlying question we are trying to answer is how to combine two different streams of information. Our goal is to model an unknown function that entails some interaction between the two variables. The standard approach is to approximate this function by a neural network. If we only use concatenation of x and z, the interaction between x and z is only additive. 

In contrast, a single layer with multiplicative interactions would impose the functional form 

![fig1](https://jasonkim8652.github.io/assets/images/MI_1.png)

where, $W$ is a 3D weight tensor, $U$, $V$ are regular weight matrtices and b is a vector. This form provide the right inductive bias to learn certain families of functions tha tare of interest in practice. 

![fig2](https://jasonkim8652.github.io/assets/images/MI_2.png)

### Hypernetworks as Multiplicaitve Interactions
A Hypernetwork Ha et al.[[1]](https://arxiv.org/abs/1609.09106) is a neural network $g$ that is used to generate the weights of another neural network given some context or input vector $z$. In the case where $f$ and $g$ are affine, such a network is exactly equivalent to the multiplicative form described above. Specifically, we can decompose equation (1) and set $W'$ as the generated 2D weight matrix and $b'$ as the generated bias from some hypernetwork. 

This allows us to have an input-conditional weight matrix and bias vector that are then used to generate output $y = W'x + b'$. We can also consider the more general case of any affine transformation being generated by some arbitrary neural network, which can also be viewed as a multiplicative interaction where we first embed the context $z$ and then use it in the equation above. 

### Diagonal Forms and Gating Mechanisms
Let us consider a diagonal approximation to the projected $W'$. Multiplying with $W'= $diag$(a_1, ..., a_n)$ can be implemented efficiently as element wise multiplication or the Hadamard product. This form now resembles commonly used gating methods, albeit they are often used with additional non-linearities. It can be viewed as a hypernetwork as well, where zW represents the function generating parameters. 

### Attention and Multiplicative Interactions
While not the focus of this work, we note that attention systems in sequence modelling similarly use multiplicative interactions to effectively scale different parts of the input. This is typically done using diagonal form above with $m = f(x,z), y = m \odot x$ where m is often a bounded mask. Attention systems are typically used with different aims to those we describe here: they can suppress or amplify certain inputs and allow long-range dependencies by combining inputs across time-steps. We use these insights to posit that while more expensive, considfering a higher order interaction (generating a vector mask) might prove more beneficial to such systems but we do not specifically consider attention in this paper and leave it to future work. 

### Scales and Biases
Further, we can make another low-rank approximation to the diagnoal form and generate insetad a scalar matrix -i.e. the hypernetwork outputs a single scalar scale(and/or bias) parameter per channel or feature vector we are considering, instead of a vector. We can again write this as $f = z^TW \odot x$ where $z^T W =  \alpha I$. 

### Multiplicative Interactiona and Metric Learning
Another highly related field of active research is that of metric learning, where one tries to find the most suitable metric to measure similarity between objects in some parametrised space of metrics. One of the most commonly used classes is that of Mahalanobis distances $d_C(x,z) = ||x-z||_C = (x-z)^TC^{-1}(x-z)$. In metric learning, however, one usually epxlicitly defines losses over tuples(or higher order n-tuples) with direct supervision, while here we consider building blocks that can learn a metric internally, without direct supervision. 

### The Taxonomy of Multiplicative Interactions
We can think of multiplicative interactions equivalently in terms of either: 
1. the approximation to the 3D tensor made
2. the output of the "projected" context by the hyper network
3. the operation used to combine the generated weights/context and the input

> we are focusing on 3rd one. 

## Expressivity of the model 

![fig3](https://jasonkim8652.github.io/assets/images/MI_3.png)

We first show that multiplicative interactions strictly enlarge the hypotheses space of vanilla MLPs - that is, we add new function which multi-layer multiplicative models can now represent perfectly, while also preserving our ability to represent those in the existing set modeled perfectly by vanilla MLPs. 

> Theorem 1. Let $H_{mlp}$ denote the hypotheses space of standard MLPs with ReLU activation function, and let $H_{mu}$ denote the hypotheses space of analogous networks, but with each linear layer replaced with a multiplicative layer, then we have $H_{mlp} \subset H_{mu}$.



## Reference
[1] Ha, David, Andrew Dai, and Quoc V. Le. "Hypernetworks." arXiv preprint arXiv:1609.09106 (2016).