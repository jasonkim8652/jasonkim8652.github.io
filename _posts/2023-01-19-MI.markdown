---
title:  "Multiplicative Interactions and Where to Find Them"
classes: wide
excerpt: "Paper review about Multiplicative interaction, which is new concept for fusing representation"
date:   2023-01-18 10:31:24 +0900
categories: 
  - AI
tags:
  - AI background
mathjax: true
---

## Introduction

> Concatenation has some problem in their expressive power when we handle two different vector. Let's study about Multiplicaitve implication that could increase expressibe power of model. 

Multiplicative interactions have reappeared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We hypothesize that multiplicative interactions are suitable for representing certain meaningufl classes of functions needed to build algorithmic operations such as conditional statements or simlarity metrics, and more generally as an effective way of intergrating contextual infromation in a network in a way that generalized effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challengning, large-scale reinforcement learning(RL) and sequence modelling tasks when a conceptually simple muiltiplicative interaction module is incorporated. 

Such improvements are consistent with our hypothesis that  the use of appropriately applied multiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. 

Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models(hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficiacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems. 

## Multiplicative Interactions

The underlying question we are trying to answer is how to combine two different streams of information. Our goal is to model an unknown function that entails some interaction between the two variables. The standard approach is to approximate this function by a neural network. If we only use concatenation of x and z, the interaction between x and z is only additive. 

In contrast, a single layer with multiplicative interactions would impose the functional form 

![fig1](https://jasonkim8652.github.io/assets/images/MI_1.png)

where, $W$ is a 3D weight tensor, $U$, $V$ are regular weight matrtices and b is a vector. This form provide the right inductive bias to learn certain families of functions tha tare of interest in practice. 

![fig2](https://jasonkim8652.github.io/assets/images/MI_2.png)

### Hypernetworks as Multiplicaitve Interactions

A Hypernetwork Ha et al.[[1]](https://arxiv.org/abs/1609.09106) is a neural network $g$ that is used to generate the weights of another neural network given some context or input vector $z$. In the case where $f$ and $g$ are affine, such a network is exactly equivalent to the multiplicative form described above. Specifically, we can decompose equation (1) and set $W'$ as the generated 2D weight matrix and $b'$ as the generated bias from some hypernetwork. 

This allows us to have an input-conditional weight matrix and bias vector that are then used to generate output $y = W'x + b'$. We can also consider the more general case of any affine transformation being generated by some arbitrary neural network, which can also be viewed as a multiplicative interaction where we first embed the context $z$ and then use it in the equation above. 

### Diagonal Forms and Gating Mechanisms

Let us consider a diagonal approximation to the projected $W'$. Multiplying with $W'= $diag$(a_1, ..., a_n)$ can be implemented efficiently as element wise multiplication or the Hadamard product. This form now resembles commonly used gating methods, albeit they are often used with additional non-linearities. It can be viewed as a hypernetwork as well, where zW represents the function generating parameters. 

### Attention and Multiplicative Interactions

While not the focus of this work, we note that attention systems in sequence modelling similarly use multiplicative interactions to effectively scale different parts of the input. This is typically done using diagonal form above with $m = f(x,z), y = m \odot x$ where m is often a bounded mask. Attention systems are typically used with different aims to those we describe here: they can suppress or amplify certain inputs and allow long-range dependencies by combining inputs across time-steps. We use these insights to posit that while more expensive, considfering a higher order interaction (generating a vector mask) might prove more beneficial to such systems but we do not specifically consider attention in this paper and leave it to future work. 

### Scales and Biases

Further, we can make another low-rank approximation to the diagnoal form and generate insetad a scalar matrix -i.e. the hypernetwork outputs a single scalar scale(and/or bias) parameter per channel or feature vector we are considering, instead of a vector. We can again write this as $f = z^TW \odot x$ where $z^T W =  \alpha I$. 

### Multiplicative Interactiona and Metric Learning

Another highly related field of active research is that of metric learning, where one tries to find the most suitable metric to measure similarity between objects in some parametrised space of metrics. One of the most commonly used classes is that of Mahalanobis distances $d_C(x,z) = ||x-z||_C = (x-z)^TC^{-1}(x-z)$. In metric learning, however, one usually epxlicitly defines losses over tuples(or higher order n-tuples) with direct supervision, while here we consider building blocks that can learn a metric internally, without direct supervision. 

### The Taxonomy of Multiplicative Interactions

We can think of multiplicative interactions equivalently in terms of either: 
1. the approximation to the 3D tensor made
2. the output of the "projected" context by the hyper network
3. the operation used to combine the generated weights/context and the input

> we are focusing on 3rd one. 

## Expressivity of the model 

![fig3](https://jasonkim8652.github.io/assets/images/MI_3.png)

We first show that multiplicative interactions strictly enlarge the hypotheses space of vanilla MLPs - that is, we add new function which multi-layer multiplicative models can now represent perfectly, while also preserving our ability to represent those in the existing set modeled perfectly by vanilla MLPs. 

> Theorem 1. Let $H_{mlp}$ denote the hypotheses space of standard MLPs with ReLU activation function, and let $H_{mu}$ denote the hypotheses space of analogous networks, but with each linear layer replaced with a multiplicative layer, then we have $H_{mlp} \subset H_{mu}$.

While valuable, such a result can also be trivially obtained by adding somewhat exotic activtion functions to the pool of typically used ones. While increasing the hypothesis space on its own is not of great significance, the crucial point here is that the set $H_{mu}\\H_{mlp}$ helps extend our coverage to the set of basic functions that one would expect to need in composing solutions that mimic systems of interest - such as logical, physical, or biological ones. 

Let's see the figure. For the gating and dot-product function classes, the complexity of MLPs required to learn them seems to grow exponenetially, while the growth for multiplicative models is quadratic. On the other hands summation is trivially easier for an MLP. Thus we do not argue that multiplicative interactions are a silver bullet -but that such interactions add an important class of functions to the hypothesis set that are often the right inductive bias, or algorithmic building block, for many kinds of problems. 

## Experimental Setup
We will show that multiplicative interactions allow better intergartion of (a) latent variables in decoder models, (b) task or contextual information in multitask learning, (c) recurrent state in sequence models. We use neural process regression, multitask RL and language modelling as exemplar domains. 

## Learning Context Dependent Layers for Multitask Learning
We start by considering the general paradigm of multitask-learning, wherein the goal is to train one model to solve K different tasks. We show that we can boost performance here by learning context or task-conditional layers with multiplicative interactions. There is generally a trade-off between the transfer from similar tasks and the negative interference between those with very different solutions. Our claim is thus that context-conditional latyers provide a best-of-both-worlds approach, with an inductive bias that allows transfer while also limiting interference. 

### Multitask RL on DMLAB-30
Next, we consider a larger sacle problem: multitask RL on the Deep Mind Lab-30 domain [[2]](https://arxiv.org/abs/1612.03801). This is a suite of 30 tasks in a partially-observable, first-person-perspective 3D environment, encompassing a range of laser-tag, naviagtion, and memory levels. We use a typical actor-critic RL setup within the Impalal framework, with multiple actors and a single learner with off-policy correction. 

#### Multi-head Policy and Value Layers
We first show that a multi-headed agent architecture with one policy and value head per level does in fact boost performance. While this does use privileged infromation, this does show that thereis some degree of interference between levels from sharing policy and value layers. 

#### Multiplicative Policy and Value Layers
We can instead consider usign multiplicative layers here to integrate task information(one-hot task ID) to modulate compute-paths. We learn a task embedding as below, and use it in a multiplicative layer that projects to policy and value functions. That is, we now have $c = relu(MLP(I_i))$as a context, and $\pi_t , V_t = Mu(h_t,c) $. 

Our hypothesis is that while multiheaded architectures reduce interference, they also remove the ability of policy-transfer between the different layers. Further, each head only gets 1/K of the number of gradient updates. 

![fig4](https://jasonkim8652.github.io/assets/images/MI_4.png)

#### Multiplicative Policies with Learnt Contexts 
We find (somewhat surprisingly) that we can get similar or greater performance gains without using any task infromation, replacing task ID $I_i$ instead with a learnt non-linear projection of the LSTM output.

## Latent Variable Models with Multiplicative Decoder
We previously demonstrated the ability of multiplicative interactions to extract and efficiently combine contextual information. We next explore the paradigm where the two streams of information begin combined refer to semantically different features. 

This is achieved by embedding context points $(x_i, y_i)$ individually with an encoder network, and then taking the mean of these embeddings. This gives latent variables z that are a representation of the function that maps x to y. A new data point x* is mappend to y* by passing $[z;x*]$ through a decoder network. 

We aim to increase the expressivity of the decoder by improving the conditioning on z. The standard approach is to concatenate x and z leading to a purely additive relationship. Instead, we replace the final layer of the MLP decoder with the multiplicative form. As an additional baseline, we conisder skip connections between the latent variable and each layer of the decoder, as a means to avoid latent variable collapse. 




## Reference
1. Ha, David, Andrew Dai, and Quoc V. Le. "Hypernetworks." arXiv preprint arXiv:1609.09106 (2016).
2. Beattie, Charles, et al. "Deepmind lab." arXiv preprint arXiv:1612.03801 (2016).