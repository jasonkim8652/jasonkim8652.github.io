---
title:  "Multiplicative Interactions and Where to Find Them"
classes: wide
excerpt: "Paper review about Multiplicative interaction, which is new concept for fusing representation"
date:   2023-01-18 10:31:24 +0900
categories: 
  - AI
tags:
  - AI background
mathjax: true
---

## Introduction

Multiplicative interactions have reappeared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We hypothesize that multiplicative interactions are suitable for representing certain meaningufl classes of functions needed to build algorithmic operations such as conditional statements or simlarity metrics, and more generally as an effective way of intergrating contextual infromation in a network in a way that generalized effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challengning, large-scale reinforcement learning(RL) and sequence modelling tasks when a conceptually simple muiltiplicative interaction module is incorporated. 

Such improvements are consistent with our hypothesis that  the use of appropriately applied multiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. 

Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models(hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficiacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems. 

## Multiplicative Interactions
The underlying question we are trying to answer is how to combine two different streams of information. Our goal is to model an unknown function that entails some interaction between the two variables. The standard approach is to approximate this function by a neural network. If we only use concatenation of x and z, the interaction between x and z is only additive. 

In contrast, a single layer with multiplicative interactions would impose the functional form 

![fig1](https://jasonkim8652.github.io/assets/images/MI_1.png)

where, $W$ is a 3D weight tensor, $U$, $V$ are regular weight matrtices and b is a vector. This form provide the right inductive bias to learn certain families of functions tha tare of interest in practice. 

![fig2](https://jasonkim8652.github.io/assets/images/MI_2.png)

### Hypernetworks as Multiplicaitve Interactions
A Hypernetwork Ha et al.[[1]](https://arxiv.org/abs/1609.09106) is a neural network $g$ that is used to generate the weights of another neural network given some context or input vector $z$. In the case where $f$ and $g$ are affine, such a network is exactly equivalent to the multiplicative form described above. Specifically, we can decompose equation (1) and set $W'$ as the generated 2D weight matrix and $b'$ as the generated bias from some hypernetwork. 

This allows us to have an input-conditional weight matrix and bias vector that are then used to generate output $y = W'x + b'$. We can also consider the more general case of any affine transformation being generated by some arbitrary neural network, which can also be viewed as a multiplicative interaction where we first embed the context $z$ and then use it in the equation above. 

### Diagonal Forms and Gating Mechanisms
Let us consider a diagonal approximation to the projected $W'$. Multiplying with $W'= diag(a_1, ..., a_n)$ can be implemented efficiently as element wise multiplication or the Hadamard product. This form now resembles commonly used gating methods, albeit they are often used with additional non-linearities. It can be viewed as a hypernetwork as well, where zW represents the function generating parameters. 

### Attention and Multiplicative Interactions
While not the focus of this work, we note that attention systems in sequence modelling similarly use multiplicative interactions to effectively scale different parts of the input. This is typically done using diagonal form above with $m = f(x,z), y = m \odot x$ where m is often a bounded mask. 

## Reference
[1] Ha, David, Andrew Dai, and Quoc V. Le. "Hypernetworks." arXiv preprint arXiv:1609.09106 (2016).